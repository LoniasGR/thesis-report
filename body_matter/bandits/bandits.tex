\chapter{\en{Bandits}}
\section{Το πρόβλημα των \en{bandits}}

Το πρόβλημα των ληστών (\en{bandits}), πήρε την ονομασία του από τα μηχανήματα του καζίνο, τους κουλοχέρηδες. Ο όρος \en{one arm bandit} προέρχεται από το γεγονός ότι οι κουλοχέρηδες έχουν ένα μοχλό-χέρι και σου "κλέβουν" τα χρήματα. Η ορολογία στα Ελληνικά δεν είναι ιδιαίτερα καθιερωμένη, οπότε και θα χρηιμοποιήσουμε την Αγγλική. 

Το πρόβλημα των \en{bandits} είναι μια απλοποίηση του προβλήματος της Ενισχυτικής Μάθησης, και στην απλότερη του μορφή το πρόβλημα δεν είναι προσεταιριστικό, δηλαδή κάθε κατάσταση θεωρείται ξεχωριστό γεγονός. Αυτό το γεγονός ότι οι αποφάσεις που κάνει ο πράκτορας στον ένα γύρο δεν επηρεάζουν τις ανταμοιβές και τις επιλογές του πράκτορα στους επόμενους γύρους, είναι και το πιο βασικό χαρακτηριστικό που μας ενδιαφέρει για να απλοποιήσουμε ελαφρώς και το δικό μας πρόβλημα. Ένα ακόμα σύνηθες χαρακτηριστικό των \en{bandits} είναι ότι ο πράκτορας μπορεί να παρατηρήσει τις ανταμοιβές του σε κάθε γύρο. Αν δεν μπορεί, τότε το πρόβλημα αυτό λέγεται πρόβλημα \textit{μερικής παρακολούθησης} και δεν είναι κάτι που θα μας απασχολήσει περαιτέρω.

Παρόλο που το πρόβλημα μοίαζει φαινομενικά να είναι πολύ απλούστερο του πλήρους προβλήματος ΕΜ, η επίλυση του δεν είναι τόσο εύκολη. Σαν παράδειγμα \cite{lattimore2020bandit}, μπορούμε να σκεφτούμε ένα κουλοχέρη που έχει δύο μοχλούς, έναν δεξιό και έναν αριστερό, τους οποίους τραβώντας τους για 10 γύρους έχουμε τα παρακάτω αποτελέσματα.

\begin{center}
    \begin{tabular}{ccccccccccc}
        \hline
         Γύρος     & 1 & 2  & 3  & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
         \hline
         Αριστερό & 0 &    & 10 & 0 &   & 0 &   &   &   & 10  \\ \\
         Δεξί     &   & 10 &    &   & 0 &   & 0 & 0 & 0 &     \\
         \hline
    \end{tabular}
\end{center}

Υπολογίζοντας την μέση ανταμοιβή που έχουμε από κάθε χέρι, μπορούμε να υπολογίσουμε ότι για το αριστερό χέρι αυτή είναι 4€, ενώ για το δεξί είναι 2€. Άρα το αριστερό χέρι είναι φαινομενικά καλύτερο. Ποιά θα ήταν η στρατιγική μας από εδώ και πέρα, αν είχαμε ακόμα 10 ακόμα προσπάθειες; Θα χρησιμοποιούσαμε μόνο το αριστερό χέρι για να εκμεταλευτούμε αυτό που πιστεύουμε ότι είναι καλύτερο; Θα χρησιμοποιούσαμε το δεξί χέρι, για να εξερευνήσουμε και να μάθουμε αν έχουμε σωστή προσέγγιση της τιμής του; Το δίλημμα μεταξή εκμετάλλευσης και εξερεύνησης είναι κεντρικό στα προβλήματα αυτά.

Η ορολογία που χρησιμοποιούμε στα προβλήματα \en{bandits} είναι η ίδια που χρησιμοποιήσαμε και στα προβλήματα ΕΜ, με την διαφορά ότι το πλήθος των γύρων που θα παίξει ο πράκτορας ονομάζονται \textbf{ορίζοντας}. Τα προβλήματα των \en{bandits} είναι προβλήματα που η δυναμική του περιβάλλοντος είναι άγνωστη, και έτσι ο πράκτορας πρέπει να την ανακαλύψει παίζοντας. Το μόνο που γνωρίζει ο πράκτορας για το περιβάλλον είναι ότι βρίσκεται σε μια εικογένεια περιβαλλόντων $\mathcal{E}$.

Κύρια μετρική της ποιότητας μιας πολιτικής είναι η μετάνοια (\en{regret}), η οποία εκφράζει την διαφορά μεταξύ των αναμενόμενων ανταμοιβών μιας πολιτικής $π$ σε $n$ γύρους, η οποία δεν είναι απαραίτητα αυτή που ακολούθησε ο πράκτορας, και των ανταμοιβών που πραγματικά πήρε ο πράκτορας σε $n$ γύρους, σύμφωνα με την πολιτική που ακολούθησε. Είναι χρήσιμο να υπολογίζουμε την μετάνοια και σε σχέση με μια οικογένεια πολιτικών $Π$. Τότε η μετάνοια είναι η διαφορά της πολιτικής που ακολούθησε ο πράκτορας σε σχέση με την πολιτική η οποία έχει τις μεγαλύτερες ανταμοιβές μεταξύ των πολιτικών της οικογένειας $Π$ (με άλλα λόγια την καλύτερη πολιτική). Συνήθως επιλέγουμε την οικογένεια $Π$, ώστε η βέλτιστη πολιτική για όλη την οικογένεια $\mathcal{E}$ να βρίσκεται μέσα στην οικογένεια πολιτικών $Π$. Ήδη διαισθητικά καταλαβαίνουμε ότι μεγάλη μετάνοια σημαίνει ότι ο πράκτορας δεν τα πάει καλά, ενώ μικρή ότι η πολιτική που ακολουθεί είναι κοντά στην βέλτιστη. 

Για να μπορέσουμε να λύσουμε το πρόβλημα, συνήθως μειώνουμε τόσο την οικογένεια πρακτόρων, όσο και την οικογένεια των περιβαλλόντων, ώστε να περιέχει στοιχεία που έχουν συγκεκριμένες επιθυμητές ιδιότητες. Στόχος κάθε φορά είναι να δημιουργήσουμε αλγορίθμους που να πετυχαίνουν όσο καλύτερη μετάνοια είναι δυνατό.

Για παράδειγμα, μια εύκολη οικογένεια προβλημάτων είναι τα στοχαστικά, χρονικά αμετάβλητα προβλήματα \en{bandits}. Σε αυτή την οικογένεια προβλημάτων, το περιβάλλον παράγει ανταμοιβές με βάση μια πράξη, οι οποίες προέρχονται από μια κατανομή η οποία είναι σχετική στην πράξη αυτή και ανεξάρτητες από τις προηγούμενες πράξεις. Επίσης οι ανταμοιβές είναι χρονικά αμετάβλητες, είναι σηναρτήσεις δηλαδή οι οποίες δεν έχουν ως παράμετρο τους τον χρόνο. 

Από την άλλη, αν δεν θέλουμε να κάνουμε καμία υπόθεση για το περιβάλλον, θα μπορούσαμε να υποθέσουμε ότι το μόνο που ξέρουμε είναι ότι οι ανταμοιβές επιλέγονται χωρίς να υπάρχει γνώση των πράξεων του πράκτορα και απλά είναι στοιχεία σε ένα πεπερασμένο σύνολο. Ουσιαστικά αυτό είναι το πρόβλημα των ανταγωνιστικών (\en{adversarial}) \en{bandits}, όπου ουσιαστικά το περιβάλλον θεωρείται αντίπαλος. Ο αντίπαλος μπορεί να έχει πολύ μεγάλη ισχύ, ακόμα και την ικανότητα να δει τον κώδικα των αλγορίθμων και να διαλέξει ανταμοιβές αντίστοιχα. Παρ'όλα αυτά το πρόβλημα αυτό δεν είναι πολύ δυσκολότερο από το στοχαστικό πρόβλημα.

Ανάμεσα στα δύο αυτά άκρα, υπάρχουν πολλές επιλογές και υποθέσεις που μπορούμε να κάνουμε σχετικά με το περιβάλλον και τις πολιτικές. 

\section{Στοχαστικοί \en{bandits}}

Πιο φορμαλιστικά ένα στοχαστικό σύστημα \en{bandits} είναι μια συλλογή απο κατανομές $v = (P_a : a \in \mathcal{A})$, όπου $\mathcal{A}$ είναι, όπως πάντα το σύνολο των δυνατών δράσεων. Το περιβάλλον και ο πράκτορας αλληλεπιδρούν διαδοχικά για $n$ γύρους. Συνήθως το πλήθος των γύρων (ο ορίζοντας) είναι πεπερασμένος, αλλά πολλές η αλληλεπίδραση είναι αέναη. Σε κάθε γύρο $t \in \{1, ..., n\}$ ο πράκτορας επιλέγει μια δράση $A_t \in \mathcal{A}$, η οποία τροφοδοτεί το περιβάλλον. Το περιβάλλον τότε επιλέγει μια ανταμοιβή $X_t \in \mathbb{R}$ από μια κατανομή $P_{A_t}$ και επιστρέφει το $X_t$ στον πράκτορα. Η αλληλεπίδραση
μεταξύ πράκτορα και περιβάλλοντος παράγει ένα μέτρο πιθανότητας στην αλληλουχία των αποτελεσμάτων $A_1, X_1, A_2, X_2, ..., A_n, X_n$. Αυτή η αλληλουχία ικανοποιεί τις παρακάτω υποθέσεις:

\begin{enumerate}
    \item Η δεσμευμένη πιθανότητα της ανταμοιβής $X_t$ δεδομένου $A_1, X_1, ..., A_{t-1}, X_{t-1}, A_t$ είναι $P_{A_t}$ που διασθητικά σημαίνει ότι το περιβάλλον παίρνει ένα δείγμα $X_t$ από την κατανομή $P_{A_t}$ στον γύρο $t$.
    \item Ο δεσμευμένος κανόνας της δράσης $A_t$ δεδομένων $A_1, X_1, ... A_{t-1}, X_{t-1}$ είναι \\$π(\cdot|A_1,X_1,...,A_{t-1},X_{t-1})$ όπου $π_1, π_2, ...$ είναι η αλληλουχία από Μαρκοβιανούς πυρήνες που περιγράφουν τον πράκτορα. Διαισθητικά αυτό σημαίνει ότι ο πράκτορας δεν μπορεί να χρησιμοποιήσει παρατηρήσεις από το μέλλον σε τρέχουσες αποφάσεις.  
\end{enumerate}

Ο στόχος του πράκτορα είναι να μεγιστοποιήσει την συνολική ανταμοιβή $S_n = \sum_{t=1}{n}X_t$ (η οποία διαφέρει ελαφρά από την ανταμοιβή του προβλήματος ΕΜ, καθώς εδώ $γ=1$). Η συνολική ανταμοιβή είναι μια τυχαία ποσότητα η οποία εξαρτάται από τις πράξεις του πράκτορα και τις ανταμοιβές που πήρε από το περιβάλλον.

Αν έχουμε την πολιτική $v = (P_a : a \in \mathcal{A})$, τότε μπορούμε να ορίσουμε το μέσο όρο κάθε χεριού
\begin{equation*}
    μ_α(v) = \int_{-\infty}^{\infty}xdP_a(x)
\end{equation*}

Τότε μπορούμε να ορίσουμε το $μ^*(v) = \max_{a\in \mathcal{A}}μ_a(v)$ ο μέγιστος μέσος όρος των χεριών.

Τότε η μετάνοια της πολιτικής $π$ σε ένα πρόβλημα \en{bandit} είναι 
\begin{equation}
    R_n(π,v) = nμ^*(v) - \mathbb{E}\left[ \sum_{t=1}^{n} X_t \right]
\end{equation}

όπου η αναμενόμενη τιμή υπολογίζεται με βάση την πιθανότητα των αποτελεσμάτων που δημιουργούνται από την αλληλεπίδραση του $π$ και του $v$.

Όλες οι πολιτικές βασίζονται στην παρατήρηση ότι για να μειώσουμε την μετάνοια, ο αλγόριθμος πρέπει να ανακαλύψει την δράση/χέρι με τον μεγαλύτερο μέσο όρο. Συνήθως αυτό σημαίνει ότι ο πράκτορας πρέπει να παίξει κάθε χέρι κάποιον αριθμό φορών, ώστε να δημιουργήσει μια εκτίμηση της μέσης τιμής του χεριού, και στην συνέχεια να παίξει το χέρι με την μεγαλύτερη τιμή. Έτσι το πρόβλημα μπορεί να συνοψιστεί ως την προσπάθεια να ανακαλύψει ο πράκτορας πόσο συχνά πρέπει να παίξει κάθε χέρι, ώστε να μπορεί με στατιστική βεβαιότητα να πει ότι έχει βρει το βέλτιστο χέρι.

\section{Ανταγωνιστικοί \en{Bandits}}

Το πλαίσιο των ανταγωνιστικών \en{bandits} έχει τις ρίζες του στην θεωρία παιγνίων. Ένα παράδειγμα ενός τέτοιου προβλήματος είναι το εξής παιχνίδι. Παίζουμε μια ένα φίλο μας ένα απλό παιχνίδι με \en{bandits}, όπου ο ορίζοντας είναι $n=1$ και έχουμε 2 δράσεις. Το παιχνίδι έχει την ακόλουθη μορφή:
\begin{itemize}
    \item Λέμε στον φίλο μας την στρατιγική με βάση την οποία θα επιλέξουμε την δράση.
    \item Ο φίλος μας διαλέγει κρυφά ανταμοιβές $x_1 \in \{0,1\}$ και $x_2 \in \{0,1\}$.
    \item Εφαρμόζουμε την στρατιγική που επιλέξαμε $A \in \{1,2\}$ και παίρνουμε ανταμοιβή $x_A$.
    \item Η μετάνοια είναι $R = \max{x_1,x_2} - x_A$
\end{itemize}

Προφανώς αν ο φίλος μας επιλέξει και τις δύο ανταμοιβές να είναι 0, τότε η μετάνοια θα είναι πάντα 0. Ο τρόπος για να είναι η στρατιγική επιτυχημένη είναι η τυχαιότητα. Μπορούμε να πούμε στον φίλο μας, "Θα επιλέξω την κίνηση $A=1$ με πιθανότητα 1/2" και η αναμενόμενη μετάνοια γίνεται $R=1/2$.  Οσο μεγαλώνει ο ορίζοντας, το πλεονέκτημα του αντιπάλου όλο και μειώνεται. 

Πιο φορμαλιστικά, αν $k>1$ ο αριθμός των χεριών, τότε ένα πρόβλημα ανταγωνιστικόυ \en{bandit} \en{k}-χεριών είναι μια αυθαίρετη σειρά από διανύσματα ανταμοιβών $(x_t)_{t=1}^n$, όπου $x_t \in [0,1]^k$. Σε κάθε γύρο ο πράκτορας διαλέγει μια κατανομή πράξεων $P_t \in P_{k-1}$. Τότε η δράση $A_t \in [k]$ είναι ένα δείγμα από την κατανομή $P_t$, και ο πράκτορας λαμβάνει ανταμοιβή $x_{tA_t}$.

Η πολιτική σε αυτή την περίπτωση είναι μια συνάρτηση $π: ([k] \times [0,1])^* \rightarrow \mathcal{P}_{k-1}$, η οποία χαρτογραφεί ακολουθίες της ιστορίας σε κατανομές πάνω σε πράξεις.
Η επίδοσης της πολιτικής $π$ σε ένα περιβάλλον $x$ μετριέται από την αναμενόμενη μετάνοια, η οποία είναι η αναμενόμενη απώλεια σε κέρδος της πολιτικής $π$ σε σχέση με την καλύτερη πολιτική που επιλέγει ένα χέρι κάθε φορά. 

\begin{equation}
R_n(π,x) = \max_{i \in [k]} \sum_{t=1}^n x_{t_i} - \mathbb{E}\left[\sum_{t=1}^n x_{tA_t}\right]   
\end{equation}
όπου η αναμενόμενη τιμή είναι πάνω στην τυχαίοτητα των πράξεων του πράκτορα.

Η μετάνοια χειρότερης περίπτωσης σε όλα τα περιβάλλοντα είναι
\begin{equation*}
    R_n^*(π) =  \sup_{x \in [0,1]^{n \times k}} R_n(π,x)
\end{equation*}
Για να φτιάξουμε πολιτικές που είναι υπογραμμικές στο $n$ στην χειρότερη περίπτωση, δηλαδή πολιτικές $π$ που ισχύει 
\begin{equation*}
    \lim_{n \to \infty} \frac{R^*_n(π)}{n} = 0
\end{equation*}
θα πρέπει να χρησιμοποιήσουμε πολιτικές με τυχαιότητα.

\section{\en{Contextual Bandits}}
Η περίπτωση \en{bandits} που μας ενδιαφέρει είναι αυτή στην οποία ο αλγόριθμος έχει πρόσβαση σε πληροφορίες σχετικά με το συγκείμενο του περιβάλλοντος, τις οποίες θα μπορούσε να χρησιμοποιήσει για να πάρει καλύτερες αποφάσεις. Το πρόβλημα και η μετρική (η μετάνοια) που μελετήσαμε προηγμένως δεν χρησιμοποιούσαν τέτοια δεδομένα και προσπαθούσαν να επιλέξουν την καλύτερη κίνηση. Το πρόβλημα αυτό στην μορφή που περιγράφουμε εδώ, μελετήθηκε από τον \en{John Langford}  και τον \en{Tong Zhang} στο \cite{langford_epoch-greedy_2007}, καθώς σε προβλήματα στον πραγματικό κόσμο πάντα υπάρχουν επιπλέον πληροφορίες που μπορεί να χρησιμοποιήσει ο πράκτορας για να πάρει καλύτερες αποφάσεις. Συγκεκριμένα το πρόβλημα που προσπαθούσαν να λύσουν είναι η αντιστοίχηση διαφημήσεων σε περιεχόμενο ιστοσελίδων στο ίντερνετ. 

Για να μελετήσουμε αυτή την νέα περίπτωση θα χρειαστεί να επεκτείνουμε το πλαίσιο στο οποίο εργαζόμαστε, καθώς για τον ορισμό της μετάνοιας, ώστε μπορέσουμε να μοντελοποιήσουμε αυτα τα προβλήματα, τα οποία περιέχουν πληροφορίες συγκειμένου. Είναι σημαντικό να έχουμε υπ'όψιν ότι όταν σχεδιάζουμε μια καινούρια μετρική έχουμε να  συμβιβαστούμε μεταξύ της μεροληψίας και της διακύμανσης \en{bias-variance trade-off}. Μεροληψίας από την άποψη ότι δεν θέλουμε να βρούμε μια κακή μετρική με την οποία να συγκριθούμε, γιατί τότε κάθε αλγόριθμος που θα έχει παρόμοια απόδοση με την μετρική θα έχει κακή απόδοση. Από την άλλη, ο συναγωνισμός με μια καλύτερη μετρική μπορεί να είναι πολύ δύσκολος από την προοπτική της μάθησης και αυτή η τιμωρία μπορεί να υπερτερεί των πλεονεκτημάτων.  

Αν προσεγγίσουμε τους \en{contextual bandits} με χρήση των ιδεών από τους ανταγωνιστικούς \en{bandits}, τότε το πρόβλημα παίρνει την παρακάτω μορφή:

\begin{enumerate}
    \item Ο αντίπαλος κρυφά διαλέγει ανταμοιβές $(x_t)_{t=1}^n$ με $x_t \in [0,1]^k$
    \item Ο αντίπαλος κρυφά διαλέγει συγκείμενο $(c_t)_{t=1}^n$ με $c_t \in \mathcal{C}$, όπου $\mathcal{C}$ είναι το σταθερό σύνολο των πιθανών συγκειμένων. 
    \item Για τους γύρους $t=1,2,...,n$:
    \begin{enumerate}
        \item Ο πράκτορας παρατηρεί συγκείμενο  $c_t \in \mathcal{C}$
        \item Ο πράκτορας διαλέγει κατανομή $P_t \in \mathcal{P}_{k-1}$ και παίρνει δείγμα $A_t$  από το $P_t$
        \item Ο πράκτορας παρατηρεί ανταμοιβή $X_t = x_{tA_t}$
    \end{enumerate}
\end{enumerate}

Το πλαίσιο των \en{contextual bandits} μας προσφέρει το εργαλείο για να περιγράψουμε το πρόβλημα των συστάσεων που θέλουμε να λύσουμε. Μετά την εισαγωγή του προβλήματος σαν μέθοδο για την επίλυση τέτοιου είδους προβλημάτων το 2007, έχει υπάρξει σημαντική έρευνα, αλλά και χρήση στην βιομηχανία τέτοιων μεθόδων, ειδικά σε περιπτώσεις που το πλήθος των αντικειμένων αλλάζει δυναμικά, πχ νέα άρθρα προστίθενται κάθε μέρα, ενώ τα παλία άρθρα είναι πλεον λιγότερο σημαντικά. Έτσι εταιρίες όπως η \en{Microsoft}, το \en{LinkedIn}, το \en{Netflix} και άλλες χρησιμοποιούν τέτοιες μεθόδους για να προτείνουν άρθρα, διαφημίσεις ή ταινίες αντίστοιχα. Το \en{Netflix} χρησιμοποιεί επίσης \en{contextual bandits} για να επιλέξει την εικόνα που δείχνει για κάθε ταινία \cite{blog_artwork_2017}.  Στα επόμενα μέρη της διπλωματικής, δεν αναλύσουμε περισσότερο τις ιδιότητες των αλγορίθμων αυτών, όυτε θα προχωρήσουμε σε αναλύσεις της μετάνοιας τους, αλλά θα τους χρησιμοποιήσουμε μόνο βάση της ανάλυσης στις εργασίες που εισήχθηκαν.