@book{rlbook,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  isbn      = {0262039249},
  publisher = {A Bradford Book},
  address   = {Cambridge, MA, USA},
  abstract  = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  langid    = {english}
}

@article{drlbs,
  author   = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal  = {IEEE Signal Processing Magazine},
  title    = {Deep Reinforcement Learning: A Brief Survey},
  year     = {2017},
  volume   = {34},
  number   = {6},
  pages    = {26-38},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  keywords = {},
  doi      = {10.1109/MSP.2017.2743240},
  issn     = {1558-0792},
  month    = {11},
  langid   = {english}
}

@misc{li2018deep,
  title         = {Deep Reinforcement Learning},
  author        = {Yuxi Li},
  year          = {2018},
  eprint        = {1810.06339},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  langid        = {english}
}

@book{aigreek,
  author    = {Ι. Βλαχάβας and Π. Κεφαλάς and Ν. Βασιλειάδης and Φ. Κόκκορας and Η. Σακελλαρίου},
  publisher = {Πανεπιστήμιο Μακεδονίας},
  title     = {Τεχνητή Νοημοσύνη},
  year      = {2006},
  langid    = {greek}
}

@misc{silver2015,
  author       = {David Silver},
  title        = {Lectures on Reinforcement Learning},
  howpublished = {\textsc{url:}~\url{https://www.davidsilver.uk/teaching/}},
  year         = {2015},
  langid       = {english}
}

@book{lattimore2020bandit,
  title     = {Bandit Algorithms},
  author    = {Lattimore, T. and Szepesv{\'a}ri, C.},
  isbn      = {9781108486828},
  lccn      = {2019053276},
  year      = {2020},
  publisher = {Cambridge University Press},
  langid    = {english}
}


@inproceedings{langford_epoch-greedy_2007,
  title     = {The {Epoch}-{Greedy} {Algorithm} for {Multi}-armed {Bandits} with {Side} {Information}},
  volume    = {20},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Langford, John and Zhang, Tong},
  editor    = {Platt, J. and Koller, D. and Singer, Y. and Roweis, S.},
  year      = {2007},
  langid    = {english}
}


@misc{blog_artwork_2017,
  title    = {Artwork {Personalization} at {Netflix}},
  url      = {https://netflixtechblog.com/artwork-personalization-c589f074ad76},
  abstract = {Artwork is the first instance of personalizing not just what we recommend but also how we recommend.},
  language = {en},
  urldate  = {2023-01-30},
  journal  = {Medium},
  author   = {Blog, Netflix Technology},
  month    = dec,
  year     = {2017},
  langid   = {english}
}

@article{turing1950computing,
  added-at            = {2012-06-19T15:53:23.000+0200},
  author              = {Turing, A. M.},
  biburl              = {https://www.bibsonomy.org/bibtex/2c6b8db241dec2cec3477ce771abebb8f/jaeschke},
  copyright           = {Copyright © 1950 Oxford University Press},
  interhash           = {3f7a151a4f79fe75b4bb148b41279a9b},
  intrahash           = {c6b8db241dec2cec3477ce771abebb8f},
  issn                = {00264423},
  journal             = {Mind},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {Oct., 1950},
  keywords            = {},
  language            = {English},
  number              = 236,
  pages               = {433--460},
  publisher           = {Oxford University Press on behalf of the Mind Association},
  series              = {New Series},
  timestamp           = {2012-06-19T15:53:23.000+0200},
  title               = {Computing Machinery and Intelligence},
  url                 = {http://www.jstor.org/stable/2251299},
  volume              = 59,
  year                = 1950,
  langid              = {english}
}

@article{recent_advances_dl_2021,
  author     = {Jinjie Ni and
                Tom Young and
                Vlad Pandelea and
                Fuzhao Xue and
                Vinay Adiga and
                Erik Cambria},
  title      = {Recent Advances in Deep Learning Based Dialogue Systems: {A} Systematic
                Survey},
  journal    = {CoRR},
  volume     = {abs/2105.04387},
  year       = {2021},
  url        = {https://arxiv.org/abs/2105.04387},
  eprinttype = {arXiv},
  eprint     = {2105.04387},
  timestamp  = {Mon, 31 May 2021 08:19:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2105-04387.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  langid     = {english}
}

@misc{policy_learning_2019,
  doi       = {10.48550/ARXIV.1906.00499},
  url       = {https://arxiv.org/abs/1906.00499},
  author    = {Zhang, Zhirui and Li, Xiujun and Gao, Jianfeng and Chen, Enhong},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Budgeted Policy Learning for Task-Oriented Dialogue Systems},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid    = {english}
}

@misc{chatgpt_2022,
  doi       = {10.48550/ARXIV.2203.02155},
  url       = {https://arxiv.org/abs/2203.02155},
  author    = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Training language models to follow instructions with human feedback},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid    = {english}
}

@misc{rl_dialogue_2016,
  doi       = {10.48550/ARXIV.1606.01541},
  url       = {https://arxiv.org/abs/1606.01541},
  author    = {Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Deep Reinforcement Learning for Dialogue Generation},
  publisher = {arXiv},
  year      = {2016},
  copyright = {Creative Commons Attribution 4.0 International},
  langid    = {english}
}

@misc{rl_recommenders_2021,
  doi       = {10.48550/ARXIV.2101.06286},
  url       = {https://arxiv.org/abs/2101.06286},
  author    = {Afsar, M. Mehdi and Crump, Trafford and Far, Behrouz},
  keywords  = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Reinforcement learning based recommender systems: A survey},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid    = {english}
}

@book{jannach_zanker_felfernig_friedrich_2010,
  place     = {Cambridge},
  title     = {Recommender Systems: An Introduction},
  doi       = {10.1017/CBO9780511763113},
  publisher = {Cambridge University Press},
  author    = {Jannach, Dietmar and Zanker, Markus and Felfernig, Alexander and Friedrich, Gerhard},
  year      = {2010},
  langid    = {english}
}

@inproceedings{7872755,
  author    = {Aditya, P. H. and Budi, I. and Munajat, Q.},
  booktitle = {2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},
  title     = {A comparative analysis of memory-based and model-based collaborative filtering on the implementation of recommender system for E-commerce in Indonesia: A case study PT X},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {303-308},
  doi       = {10.1109/ICACSIS.2016.7872755},
  langid    = {english}
}


@article{yan2022bandit,
  title   = {Bandits for Recommender Systems},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2022},
  month   = {May},
  url     = {https://eugeneyan.com/writing/bandits/},
  langid  = {english}
}

@article{thompsonsampling,
  doi       = {10.48550/ARXIV.1707.02038},
  url       = {https://arxiv.org/abs/1707.02038},
  author    = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {A Tutorial on Thompson Sampling},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid    = {english}
}

@inproceedings{Zhao_2018,
  doi       = {10.1145/3219819.3219886},
  url       = {https://doi.org/10.1145%2F3219819.3219886},
  year      = {2018},
  month     = {jul},
  publisher = {ACM},
  author    = {Xiangyu Zhao and Liang Zhang and Zhuoye Ding and Long Xia and Jiliang Tang and Dawei Yin},
  title     = {Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&} Data Mining},
  langid    = {english}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space},
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{attention2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
